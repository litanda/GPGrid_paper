\section{Introduction}

Theoretical stellar model has been developed for decades to simulate star structure and evolution. However, modelling of stars are mostly carried out with sparse model grid \citep[e.g.][]{2016ApJ...823..102C} because massive computation can be time-consuming. Moreover, stellar model contents adjusted input parameters (e.g. the mixing-length parameter). Varying one of these adjusted parameter adds on an input demission and hence exponentially increasing the computational cost. A comprehensive and fine stellar model grid is hence expensive. 

When modelling star, a sparse grid is not ideal for statistics. Classical method like interpolation is one option to overcome this disadvantage. For instance, \citet{2016ApJS..222....8D} developed a method to transform stellar evolution tracks onto a uniform basis and then interpolate to construct stellar isochrones. More recently, \citet{2019MNRAS.484..771R} uses Bayesian statistics and a Markov Chain Monte Carlo approach to find a representative set of interpolated models from a grid. Both works achieve good accuracy for 3-demission gird (inputs are mass, age, and metallicity). However, the interpolation approach become less reliable in higher demissions and it hence limits the variety of input physics. The algorithm is another approach that has been used in stellar codes \citep[e.g.][]{2013ApJS..208....4P}. It offers an automated likelihood minimisation to search for optimal solutions. This method is statistically sound and works fairly well for modelling individual stars. However, it become much less efficient for modelling a large sample, because the algorithm needs to repeat computing stellar tracks many time for each single star.     

Machine learning is being applied to 	stellar researches recently. \citet{2018MNRAS.476.3233H} developed a convolutional neural network classifier for solar oscillations in red giants. \citet{2019MNRAS.484.5315W} determined masses and ages for massive RGB stars from their spectra with a machine-learning method based on kernel principal component analysis.  {(\bf Guy: can you add more relevant papers?)}

A machine-learning algorithm that involves a Gaussian process (GP) measures the similarity between data points (the kernel function) to predict values for unseen points from training data. {\bf more intro about GP}. 

The aim of this paper is training GP models to turn a sparse model grid into a continuous function and augmenting the model grid. The paper is organised as follow. We describe the computation of a representative stellar grid in Section \ref{sec:grid}. We then introduce the underline theory of GP and set up GP models for training the stellar grid in Section \ref{sec:gpmodel}. Section \ref{sec:results} demonstrates the results of GP predictions and we analyse the systematic uncertainties of GP models. We subsequently augment the stellar grid and transform it to a set of continuously-sampled models, and we model 100 fake stars with GP-trained models for testing the accuracy of our method. 
Finally we discuss the limitations of this approach, highlight areas where improvements can be found in the near future, and summary conclusions in Section \ref{sec:conclusion}.

% Set the context of the work.
% Cite relevant earlier studies

%% Lots of work on estimating stellar properties where observables are compared with stellar models.  Typical approach is grid based.  Lots of citations.  

%% Observables can come from all over.  Spectroscopic surveys (APOGEE, Galah, LAMOST, Gaia ESO, +), Astrometric Gaia, Photometric variability CoRoT, Kepler, K2, TESS, soon PLATO.

%% Lots of different models available with lots of different flavours - ask Tanda ...

%% Typical parameters to vary can refer to the star (mass, age, [Fe/H], Y_i) or they can refer to the model (MLT, overshoot, diffusion).  Most studies, certainly for field stars, treat all parameters as being independent.  

% Describe the problem we aim to solve

%% Plenty of work exists on HBM models in astro (cite fest).  By pooling together parameters we can win - for example EB's/cluster age, chemical comp.  But also we could pool parameters of the models MLT, Ov.  If we take a Bayesian approach the pooled constraint on MLT or Ov has the ability to constrain stellar parameters (e.g., age, mass).  The posterior distribution is a joint distribution!

%% Curent limitation is that this is all very tricky computationally.  Great news though - breakthroughs in machine learning, sampling methods, and GPU implementation means we now have a shot at doing this.  In this paper we give a deminstration of principle for one way of proceeding.

% Layout of this paper ...

%% 
