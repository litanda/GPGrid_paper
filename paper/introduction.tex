\section{Introduction}

Theoretical stellar model has been developed for decades to simulate star structure and evolution. However, star modelling is mostly based on sparse stellar grids \citep[e.g.][]{2016ApJ...823..102C} because massive computations are time-consuming. Moreover, stellar model contents adjusted input parameters (e.g. the mixing-length parameter). Varying one of these adjusted parameter adds on an input demission and hence exponentially increases the computational cost. A comprehensive and fine stellar model grid is hence expensive. 

A sparse grid is not ideal in terms of the statistics. Classical method like interpolation has been applied overcome this disadvantage. For instance, \citet{2016ApJS..222....8D} developed a method to transform stellar evolution tracks onto a uniform basis and then interpolate to construct stellar isochrones. More recently, \citet{2019MNRAS.484..771R} uses Bayesian statistics and a Markov Chain Monte Carlo approach to find a representative set of interpolated models from a grid. The interpolation of both works achieve good accuracy for 3-demission girds (inputs are mass, age, and metallicity). However, this approach becomes less reliable in higher demissions and it hence limits the flexibility for varying input physics. The algorithm is another approach that has been used in stellar codes \citep[e.g.][]{2013ApJS..208....4P}. It offers an automated likelihood minimisation to search for optimal solutions. This method is statistically sound and works fairly well for modelling individual stars. However, it becomes much less efficient while modelling a large sample, because the algorithm needs to iteratively compute stellar tracks many time for each single star.     

Machine learning is being applied to the field of stellar research. \citet{2018MNRAS.476.3233H} developed a convolutional neural network classifier for solar oscillations in red giants. \citet{2019MNRAS.484.5315W} determined masses and ages for massive RGB stars from their spectra with a machine-learning method based on kernel principal component analysis.  {(\bf Guy: can you add more relevant papers?)}

A machine-learning algorithm that involves a Gaussian process (GP) measures the similarity between data points (the kernel function) to predict values for unseen points from training data. {\bf more intro about GP}. 

The aim of this paper is training GP models to turn a sparse stellar grid into a continuous function and augmenting the grid. The paper is organised as follow. We describe the computation of a representative stellar grid in Section \ref{sec:grid}. We then introduce the underline theory of GP and set up the training for GP in Section \ref{sec:gpmodel}. Section \ref{sec:results} demonstrates the results of GP predictions and we analyse the systematic uncertainties. We subsequently augment the stellar grid,  present a set of continuously-sampled models, and model 100 fake stars with these GP-trained models for testing the accuracy of our method in Section \ref{sec:augmentation}. 
Finally we discuss advantages and limitations of this approach, highlight areas where improvements can be found in the near future, and summary conclusions in Section \ref{sec:conclusion}.

% Set the context of the work.
% Cite relevant earlier studies

%% Lots of work on estimating stellar properties where observables are compared with stellar models.  Typical approach is grid based.  Lots of citations.  

%% Observables can come from all over.  Spectroscopic surveys (APOGEE, Galah, LAMOST, Gaia ESO, +), Astrometric Gaia, Photometric variability CoRoT, Kepler, K2, TESS, soon PLATO.

%% Lots of different models available with lots of different flavours - ask Tanda ...

%% Typical parameters to vary can refer to the star (mass, age, [Fe/H], Y_i) or they can refer to the model (MLT, overshoot, diffusion).  Most studies, certainly for field stars, treat all parameters as being independent.  

% Describe the problem we aim to solve

%% Plenty of work exists on HBM models in astro (cite fest).  By pooling together parameters we can win - for example EB's/cluster age, chemical comp.  But also we could pool parameters of the models MLT, Ov.  If we take a Bayesian approach the pooled constraint on MLT or Ov has the ability to constrain stellar parameters (e.g., age, mass).  The posterior distribution is a joint distribution!

%% Curent limitation is that this is all very tricky computationally.  Great news though - breakthroughs in machine learning, sampling methods, and GPU implementation means we now have a shot at doing this.  In this paper we give a deminstration of principle for one way of proceeding.

% Layout of this paper ...

%% 
