\section{Introduction}

Theoretical stellar model has been developed for decades to simulate star structure and evolution. Star modelling is mostly grid-based \citep[e.g.][]{2016ApJ...823..102C} because computing many stellar models are time-consuming especially when a number of free input parameters are considered. Varying one of these adjusted parameters (mass, metallicity, helium fraction, mixing-length parameter, etc.) adds on an input demission and hence exponentially increases the computational cost. 

A sparse grid is not ideal for the statistics analysis. Classical method like interpolation has been applied to overcome this disadvantage. For instance, \citet{2016ApJS..222....8D} developed a method to transform stellar evolution tracks onto a uniform basis and then interpolate to construct stellar isochrones. More recently, \citet{2019MNRAS.484..771R} uses Bayesian statistics and a Markov Chain Monte Carlo approach to find a representative set of interpolated models from a grid. The interpolation of both works achieve good accuracy for 3-demission girds (inputs are mass, age, and metallicity). However, this approach becomes less reliable for high-demission grid. 
%The algorithm is another {\bf statistically-sound approach to model stars} \citep[e.g. the \textsc{MESA Simplex} module][]{2013ApJS..208....4P}. It offers an automated likelihood minimisation to search for optimal solutions. This method works well for modelling individual stars but is not efficient because the algorithm needs to iteratively compute stellar tracks many time. {\bf Hence, the  algorithm approach is not a good choice for modelling a large sample of stars.} 

Machine learning is being applied to the field of stellar research in many ways to efficiently characterise stars.
\citet{2016MNRAS.461.4206V} applied artificial neural network, which is a series of algorithms that endeavours to recognise underlying relationships in a set of data, to determine the evolutionary parameters of the sun and sun-like stars based on spectroscopic and seismic measurements. Using a similar artificial neural network interference, \citet{2019PASP..131j8001H} developed a method to provide the optimal starting point of model competitions for more detailed forward asteroseismic modelling. Moreover, \citet{2021arXiv210313394M} trained neural networks to predict theoretical pulsation periods of high-order gravity modes, as well as the luminosity, effective temperature, and surface gravity for a given mass, age, overshooting parameter, diffusive envelope mixing, metallicity, and near-core rotation frequency. 
%
Using different machine learning tools, \citet{2016ApJ...830...31B} trained a random forest regressors \citep{ho1995random}, which is an ensemble learning method for regression and operates by constructing a multitude of decision trees, to rapidly estimate fundamental parameters of solar-like stars based on classical and asteroseismic observations. \citet{2018MNRAS.476.3233H} developed a convolutional neural network classifier that analyses visual features in asteroseismic frequency spectra to distinguish between red giant branch stars and helium-core burning stars. \citet{2019MNRAS.484.5315W} determined masses and ages for massive RGB stars from their spectra with a machine-learning method based on kernel principal component analysis, which is a nonlinear form of principal component analysis using integral operator kernel functions and can efficiently compute principal components in high dimensional feature spaces related to input space by some nonlinear map \citep{scholkopf1997kernel}. \citet{2020MNRAS.499.2445H} applied the mixture density network \citep{bishop1994mixture}, which learns a transformation from a set of input variables to a set of output variable, to determine stars' fundamental parameters like mass and age based on observed mode frequencies, spectroscopic, and global seismic parameters.


In above studies, the discriminative machine-learning model is mostly used. The discriminative model treats observables as given facts to directly infer star fundamental parameters. The method is efficient and easy for computation, while the downside is not allowing any priors for star properties like mass.
%
In an opposite direction, the generative machine-learning model uses the star fundamental parameters as given facts to predict observables. This approach offers flexibility to prior fundamental parameters in the sampling. For instance,  \citet{2021MNRAS.tmp.1343L}  determined initial helium fraction and mixing-length parameters for a sample of {\em Kepler} dwarfs and subgiants with an artificial neural network to provide the generative model. This allowed them to prescribe prior distributions over the fundamental stellar parameters and, by extension, over population-level parameters such as a helium enrichment law. Priors encode our current knowledge and assumptions into inference from new data. This is especially important with noisy observations which span a large portion of parameter-space.


Constructing a comprehensive and fine model grid is computationally expensive. In this work, we aim to apply the machine learning tool to transform a sparse model grid onto a continuous function. We apply a machine learning algorithm that involves a Gaussian process (GP) that measures the similarity between data points (i.e., the kernel function) to predict values for unseen points from training data. We use the generative model and treat fundamental parameters as given facts to predict observables. This gives us flexibility to prior fundamental inputs when modelling stars. We organise the rest of the paper as follow. Section \ref{sec:grid} contents descriptions about the computation of a representative stellar model grid. We then introduce the underline theory of GP and the setup of GP model in Section \ref{sec:gpmodel}. We then demonstrate some preliminary studies for low-demission problems in Section~\ref{examples}. Section \ref{sec:results} demonstrates GP predictions and their systematic uncertainties. Subsequently, we augment the grid to have a set of continuously-sampled stellar models and model 100 fake stars for testing the accuracy of our method in Section \ref{sec:augmentation}. Lastly, we discuss advantages and limitations of this approach, highlight areas where improvements can be found in the near future, and summary conclusions in Section \ref{sec:conclusion}.

% Set the context of the work.
% Cite relevant earlier studies

%% Lots of work on estimating stellar properties where observables are compared with stellar models.  Typical approach is grid based.  Lots of citations.  

%% Observables can come from all over.  Spectroscopic surveys (APOGEE, Galah, LAMOST, Gaia ESO, +), Astrometric Gaia, Photometric variability CoRoT, Kepler, K2, TESS, soon PLATO.

%% Lots of different models available with lots of different flavours - ask Tanda ...

%% Typical parameters to vary can refer to the star (mass, age, [Fe/H], Y_i) or they can refer to the model (MLT, overshoot, diffusion).  Most studies, certainly for field stars, treat all parameters as being independent.  

% Describe the problem we aim to solve

%% Plenty of work exists on HBM models in astro (cite fest).  By pooling together parameters we can win - for example EB's/cluster age, chemical comp.  But also we could pool parameters of the models MLT, Ov.  If we take a Bayesian approach the pooled constraint on MLT or Ov has the ability to constrain stellar parameters (e.g., age, mass).  The posterior distribution is a joint distribution!

%% Curent limitation is that this is all very tricky computationally.  Great news though - breakthroughs in machine learning, sampling methods, and GPU implementation means we now have a shot at doing this.  In this paper we give a deminstration of principle for one way of proceeding.

% Layout of this paper ...

%% 
