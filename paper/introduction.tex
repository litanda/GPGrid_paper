\section{Introduction}

Theoretical stellar model has been developed for decades to simulate star structure and evolution. However, star modelling is mostly based on sparse stellar grids \citep[e.g.][]{2016ApJ...823..102C} because model computations are time-consuming. Moreover, stellar model contents adjusted input parameters (e.g. the mixing-length parameter). Varying one of these adjusted parameter adds on an input demission and hence exponentially increases the computational cost. A comprehensive and fine stellar model grid is hence expensive. 

A sparse grid is not ideal for the statistics analysis. Classical method like interpolation has been applied overcome this disadvantage. For instance, \citet{2016ApJS..222....8D} developed a method to transform stellar evolution tracks onto a uniform basis and then interpolate to construct stellar isochrones. More recently, \citet{2019MNRAS.484..771R} uses Bayesian statistics and a Markov Chain Monte Carlo approach to find a representative set of interpolated models from a grid. The interpolation of both works achieve good accuracy for 3-demission girds (inputs are mass, age, and metallicity). However, this approach becomes less reliable in higher demissions and it hence limits the flexibility for varying input physics. The algorithm is another approach that has been used in stellar codes \citep[e.g.][]{2013ApJS..208....4P}. It offers an automated likelihood minimisation to search for optimal solutions. This method is statistically sound and works fairly well for modelling individual stars. However, it becomes much less efficient while modelling a large sample, because the algorithm needs to iteratively compute stellar tracks many time for each single star.     

Machine learning is being applied to the field of stellar research in many ways to efficiently characterise stars.
\citet{2016MNRAS.461.4206V} applied artificial neural networks \citep{bain1873mind,james1890principles}, i.e., a series of algorithms that endeavours to recognise underlying relationships in a set of data, and successfully determined the evolutionary parameters of the sun and sun-like stars based on spectroscopic and seismic measurements. Based on a similar artificial neural network interference, \citet{2019PASP..131j8001H} developed a method to achieve optimal initiation of CPU-intensive computations in a 6-dimensional parameter space. The method provides the optimal starting point for further and more detailed forward asteroseismic modelling for the core-hydrogen-burning stage of intermediate-mass and high-mass stars. Another example is that \citet{2021arXiv210313394M} trained neural networks to predict theoretical pulsation periods of high-order gravity modes, as well as the luminosity, effective temperature, and surface gravity for a given mass, age, overshooting parameter, diffusive envelope mixing, metallicity, and near-core rotation frequency. They compared neural-network predictions with observations to constrain a sample of 37 $\gamma$ Doradus stars.
%
Using different machine learning tools, \citet{2016ApJ...830...31B} trained a random forest regressors\citep{ho1995random}, which is an ensemble learning method for regression and operates by constructing a multitude of decision trees, to  rapidly estimating fundamental parameters of main-sequence solar-like stars from classical and asteroseismic observations. \citet{2018MNRAS.476.3233H} developed a convolutional neural network classifier that analyses visual features in asteroseismic frequency spectra to distinguish between red giant branch stars and helium-core burning stars. \citet{2019MNRAS.484.5315W} determined masses and ages for massive RGB stars from their spectra with a machine-learning method based on kernel principal component analysis, which is a nonlinear form of principal component analysis using integral operator kernel functions and can efficiently compute principal components in high dimensional feature spaces related to input space by some nonlinear map \citep{scholkopf1997kernel}. \citet{2020MNRAS.499.2445H} applied the mixture density network \citep{bishop1994mixture}, which learns a transformation from a set of input variables to a set of output variable, to determine stars' fundamental parameters like mass and age based on observed mode frequencies, spectroscopic and global seismic parameters.
%
It can be noted that the discriminative machine learning model is mostly used in above studies. The discriminative model treats observables as given facts to directly infer star properties. The method is efficient and easy for computation, however, the downside is not allowing any priors for the star properties in the Bayesian framework. 
%
The generative model treats the data set in an opposite direction, says that, using the star fundamental parameters as given facts to predict observables. Predicted observables are then compared with observations to give posterior distributions for fundamental parameters. This approach offers flexibility to prior fundamental parameters. For instance, \citet{2021MNRAS.tmp.1343L} determined the initial helium fraction and the mixing-length parameters for a sample of Kepler dwarfs and subgiants based on the artificial neural network. The application of the generative model allow them giving either weakly or strongly informative priors whlie constraining the two fundamental parameters. 

Because constructing a dense model grid is computationally expensive, we aim to apply the machine learning tool to transform a sparse model grid onto a continuous function, says that, to augment a model grid. We use a different machine learning algorithm that involves a Gaussian process (GP) to map between the stellar fundamental parameters and the observables. The Gaussian process measures the similarity between data points (i.e., the kernel function) to predict values for unseen points from training data. Because the generative model is advanced in terms of the priority, and we treat fundamental parameters as given facts. 

We organise the rest of the paper as follow. Section \ref{sec:grid} contents descriptions about the computation of a representative stellar model grid. We then introduce the underline theory of GP and set up the training for GP in Section \ref{sec:gpmodel}. Section \ref{sec:results} demonstrates the results of GP predictions and we analyse the systematic uncertainties. We subsequently augment the stellar grid,  present a set of continuously-sampled models, and model 100 fake stars with these GP-trained models for testing the accuracy of our method in Section \ref{sec:augmentation}. 
Finally we discuss advantages and limitations of this approach, highlight areas where improvements can be found in the near future, and summary conclusions in Section \ref{sec:conclusion}.

% Set the context of the work.
% Cite relevant earlier studies

%% Lots of work on estimating stellar properties where observables are compared with stellar models.  Typical approach is grid based.  Lots of citations.  

%% Observables can come from all over.  Spectroscopic surveys (APOGEE, Galah, LAMOST, Gaia ESO, +), Astrometric Gaia, Photometric variability CoRoT, Kepler, K2, TESS, soon PLATO.

%% Lots of different models available with lots of different flavours - ask Tanda ...

%% Typical parameters to vary can refer to the star (mass, age, [Fe/H], Y_i) or they can refer to the model (MLT, overshoot, diffusion).  Most studies, certainly for field stars, treat all parameters as being independent.  

% Describe the problem we aim to solve

%% Plenty of work exists on HBM models in astro (cite fest).  By pooling together parameters we can win - for example EB's/cluster age, chemical comp.  But also we could pool parameters of the models MLT, Ov.  If we take a Bayesian approach the pooled constraint on MLT or Ov has the ability to constrain stellar parameters (e.g., age, mass).  The posterior distribution is a joint distribution!

%% Curent limitation is that this is all very tricky computationally.  Great news though - breakthroughs in machine learning, sampling methods, and GPU implementation means we now have a shot at doing this.  In this paper we give a deminstration of principle for one way of proceeding.

% Layout of this paper ...

%% 
