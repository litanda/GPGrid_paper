\section{Discussion and Conclusions (work with Guy)}\label{sec:conclusion}

In this work, we apply a machine-learning algorithm that involves a Gaussian process to augment a stellar grid. We train GP models for the stellar grid and obtain continuous functions that map fundamental inputs to observable outputs on a good accuracy level. We also train another set of GP models which describe the local systematic uncertainty of each output parameter. A GP-trained model set is then generated and we use it to model fake stars. The GP-based modelling is obviously advanced in modelling individual stars compared with the gird-based modelling because it provides continuous sampling. We lastly test GP-determined masses and ages by comparing them with true values and find good consistence. 

1. summary of what paper is done.
/what we did
/how it works
/consequence, we have better accuracy reflects the truth of uncertainty. 


2. Details of the GP/ comments on the application of GP: kernels, mean function/ training and prediction/uncertainty issue/how we fix that/

3. with the trained GP, we augment sparse grid. We fit stars. We get better age estimates, for other parameters, which are known to be sparse, can be properly characterised thank to the fully sampling. 

%\subsubsection*{Advantages}
%GP is a very efficient tool to augment a stellar grid instead of computing massive evolutionary tracks. For the case in this study, we train 10 GP-Grid models and 1 GP-SYS model for each output parameter and hence 55 GP models in total. The training process takes approximate one hour per GP model on a NVidia Tesla V100 graphics processing unit (GPU). 
%
%We also show that GP is able to manage the augmentation in high-demissions data (5 demissions in this work), which is very difficult for the interpolation approach.
%
%Moreover, the section scenario overcomes the limitation of training data size (20,000 for this case) and offers flexibility to apply GP on stellar grids with different size. 
%
%When modelling individual stars, GP significantly improves the sampling especially for initial metallicity, helium fraction, and the mixing-length parameter, which are always well spaced in a stellar grid. 
%

%{\bf Any more points?}    

%\subsubsection*{Limitations}
%GP is efficient for training global parameters but not for training the stellar structure. It hence not able to replace a stellar model. To obtain comprehensive stellar model, an optimal path would be using GP to constrain the range of fundamental parameters and then computing models with stellar code in that range.
%

%{\bf add more limitations}  

%\subsubsection*{Future works}

%This work presents an application of GP on augmenting the whole grid. However, this method is not efficient when modelling a single star. Our future work is developing a fast tool that only trains a small set of models in the grid around a star. This trained-GP model can be easily sticked with an Markov chain Monte Carlo simulator for a delicate bayesian analysis. 
%

%{\bf more future works} 