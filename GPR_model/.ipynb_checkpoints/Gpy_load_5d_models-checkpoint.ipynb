{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is for loading GPR models and predicting observables.\n",
    "# variables in this notebook:\n",
    "- gpms: GP models \n",
    "- gpps: GP predictions\n",
    "- sys: systematical uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib #\n",
    "from numpy import *\n",
    "from matplotlib import *\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import os, sys\n",
    "import csv\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import corner\n",
    "\n",
    "\n",
    "\n",
    "# The lines below are specific to the notebook format\n",
    "textsize = 25\n",
    "\n",
    "%matplotlib inline\n",
    "params = {'legend.fontsize': textsize,\n",
    "          'figure.figsize': (12, 10),\n",
    "         'axes.labelsize': textsize,\n",
    "         'axes.titlesize':textsize,\n",
    "         'xtick.labelsize':textsize,\n",
    "         'ytick.labelsize':textsize,\n",
    "         'font.size': textsize}\n",
    "matplotlib.rcParams.update(params)\n",
    "plt = matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The global_kernel class is to find GP kernels which are suitable for global stellar parameters (e.g. Teff, Radius) \n",
    "#and resample them as a function of the age. \n",
    "class global_md_kernel:\n",
    "    '''\n",
    "    The global_kernel class aims to find an proper and efficient GP kernel for a global parameter (e. g. Teff) for\n",
    "    the whole grid.  \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, datapath = None, savepath = None):\n",
    "        return None\n",
    "    \n",
    "    def set_path(self, datapath = None,valipath = None, savepath = None):\n",
    "        self._datapath = datapath\n",
    "        self._valipath = valipath\n",
    "        self._savepath = savepath\n",
    "        \n",
    "        if not os.path.exists(datapath):\n",
    "            raise Warning(f'datapath:' + datapath + ' does not exist')\n",
    "\n",
    "        if not os.path.exists(valipath):\n",
    "            raise Warning(f'valipath:' + valipath + ' does not exist')\n",
    "        \n",
    "        if not os.path.exists(savepath): os.makedirs(savepath)\n",
    "        \n",
    "        print('Data path is set as' + self._datapath)\n",
    "        print('Validation path is set as' + self._valipath)\n",
    "        print('Save path is set as' + self._savepath)\n",
    "        return self\n",
    "    \n",
    "    #############################################################################################################\n",
    "    ################Change this function for different data formats##############################################\n",
    "    #############################################################################################################\n",
    "    def get_data_of_a_grid(self, path = None,\n",
    "                           condition = None, number = None,\n",
    "                           columns = None,\n",
    "                           ranges = None,\n",
    "                           normalization = None):\n",
    "        if path == None:\n",
    "            raise Warning(f'$path$ must be given')\n",
    "\n",
    "        if condition == None:\n",
    "            warnings.warn(f'$condition$ is missing, all csv files in datapath will be used', UserWarning)\n",
    "            condition = \"*.csv\"\n",
    "        if number == None:\n",
    "            warnings.warn(f'$number$ is missing, all file will be used', UserWarning)\n",
    "            number = 9999999999\n",
    "            \n",
    "        if (columns == None) or (ranges == None): \n",
    "            raise Warning(f'$columns$ and $ranges$ must be given')\n",
    "        \n",
    "        #if (validation_frac == None):\n",
    "        #    warnings.warn(f'$validation_frac$ is missing, 0.2 will be used', UserWarning)\n",
    "        #    validation_frac = 0.2\n",
    "    \n",
    "        all_files = glob.glob(path + condition)\n",
    "        random.shuffle(all_files)\n",
    "        n = min([int(number), len(all_files)])\n",
    "        files= all_files[0:n]\n",
    "        \n",
    "        print(str(n) + ' tracks are found')\n",
    "        \n",
    "        #print('the columns names are' + str(columns))\n",
    "        \n",
    "        df1 = []\n",
    "        \n",
    "        for filename in files:\n",
    "            df0 = self.get_data_of_a_track(filename = filename, \n",
    "                                           columns = columns,\n",
    "                                           ranges = ranges,\n",
    "                                           normalization =normalization)\n",
    "            df1.append(df0)\n",
    "        \n",
    "        df = []    \n",
    "        df = pd.concat(df1, ignore_index=True)\n",
    "        \n",
    "        #random_state = 1\n",
    "\n",
    "        #if (len(df['type'])*(1 - validation_frac) >= 10000):\n",
    "        #    train = df.sample( n = 10000, random_state=random_state) #random state is a seed value\n",
    "        #else:\n",
    "        #    train = df.sample(frac = (1 - validation_frac), random_state=random_state) #random state is a seed value\n",
    "        \n",
    "        #df['type'].iloc[train.index] = 'data'\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_data_of_a_track(self, filename = None, columns = None, ranges = None, normalization = None):\n",
    "        '''\n",
    "        '''        \n",
    "        one_track = []\n",
    "        one_track = pd.read_csv(filename)\n",
    "        \n",
    "        #m = re.search('yinit(.+?)MLT', filename)\n",
    "        #if m:\n",
    "        #    found = m.group(1)\n",
    "        \n",
    "        #one_track['initial_Yinit'] = float(found)                \n",
    "        #get rid of the pre-MS\n",
    "        #one_track = one_track.loc[one_track['center_h1'] <= 0.997*np.max(one_track['center_h1']) ]\n",
    "        one_track = one_track[columns]\n",
    "        #one_track['type'] = 'data'\n",
    "        one_track['hrgradient'] = ((np.gradient(one_track['effective_T']/5777.0))**2.0 \n",
    "                                 + (np.gradient(one_track['log_g']/4.44))**2.0\n",
    "                                  )**0.5\n",
    "        \n",
    "        one_track['fagegradient'] = np.gradient(one_track['frac_age'])\n",
    "      \n",
    "        for i in range(len(columns)):\n",
    "            #print(columns[i], ranges[i])\n",
    "            temp = None\n",
    "            temp = one_track[(one_track[columns[i]] >= min(ranges[i])) & (one_track[columns[i]] <= max(ranges[i]))]\n",
    "            if (temp.shape[0] >= 10):\n",
    "                one_track = one_track.loc[temp.index.min(): temp.index.max()]\n",
    "                one_track['frac_age'] = one_track['star_age']/np.max(one_track['star_age'])\n",
    "            else:\n",
    "                one_track = None\n",
    "                break\n",
    "            if (normalization != None):\n",
    "                if (normalization[i] > -999):\n",
    "                    one_track[columns[i]] = one_track[columns[i]]/normalization[i]\n",
    "        return one_track\n",
    "    \n",
    "    ##################### Plot section########################################################################################\n",
    "    \n",
    "    def preview_2d_data(self, condition = None, number = None,\n",
    "                        x1 = None, x2 = None ,y = None, \n",
    "                        x1log = None, x2log = None, ylog = None, \n",
    "                        x1normalization = None, x2normalization = None, ynormalization = None,\n",
    "                        savefig = None):\n",
    "        \n",
    "    \n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        plt.xlabel(x)\n",
    "        plt.ylabel(x2)\n",
    "        plt.title('Preview of ' + x + ' vs ' + x2 + ' color: ' + y)\n",
    "        for filename in files:\n",
    "            print(filename)\n",
    "            gpx1, gpx2, gpy, gpx1_v, gpx2_v, gpy_v, x1range, x2range, yrange = \\\n",
    "            self.get_data_of_a_track(filename, x1, x2, y, x1log, x2log, ylog, \\\n",
    "                                     x1normalization, x2normalization, \n",
    "                                     ynormalization, fraction = 0.9)\n",
    "            plt.scatter(gpx1, gpx2, c = gpy)\n",
    "        if (savefig == True): plt.savefig(self._savepath + 'S00_' + x1 + '_vs_' + x2 + '_vs_' + y + 'preview.png')\n",
    "        return None\n",
    "    \n",
    "    def plot_3d_data(self, x = None, y = None, z = None, c = None):\n",
    "        fig = plt.figure()\n",
    "        ax = Axes3D(fig)\n",
    "        cp = ax.scatter(x, y, z, c = c, s=10)\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('y')\n",
    "        ax.set_zlabel('z')\n",
    "        colorbar = plt.colorbar(cp)\n",
    "        colorbar.set_label('c')\n",
    "        return ax, colorbar \n",
    "    \n",
    "    \n",
    "   \n",
    "      #####################Data generating#################\n",
    "    def generate_2d_surface(self, xv = None, xvstep = None, xf1 = None, xf2 = None, number = None, factor = None):\n",
    "    \n",
    "        new_xv = []\n",
    "        new_xf1 = []\n",
    "        new_xf2 = []\n",
    "    \n",
    "        if factor == None: factor = 10\n",
    "        \n",
    "        number = min([factor*len(xv), number])\n",
    "    \n",
    "        for i in range(int(factor)):\n",
    "            aa = xv + xvstep*(np.random.random(len(xv))-0.5)\n",
    "            new_xv = np.concatenate((new_xv, aa))\n",
    "            new_xf1 = np.concatenate((new_xf1, xf1))\n",
    "            new_xf2 = np.concatenate((new_xf2, xf2))\n",
    "            if (len(new_xv) >= number): break\n",
    "    \n",
    "        idxs = np.random.choice(arange(len(new_xv)), number)                           \n",
    "    \n",
    "    ######################GP models##################\n",
    "    \n",
    "    def kernel_bank(self, kname,input_dim):\n",
    "        if kname == 'RBF': \n",
    "            k = GPy.kern.RBF(input_dim=input_dim, variance=1., lengthscale=1., ARD = True)\n",
    "        elif kname == 'EXP': \n",
    "            k = GPy.kern.Exponential(input_dim=input_dim, variance=1., lengthscale=1., ARD = True)\n",
    "        elif kname == 'MLP': \n",
    "            k = GPy.kern.MLP(input_dim=input_dim, ARD = True)\n",
    "        elif kname == 'MLP+RBF':\n",
    "            k1 = GPy.kern.MLP(input_dim=input_dim, ARD = True)\n",
    "            k2 = GPy.kern.RBF(input_dim=input_dim, variance=1., lengthscale=1., ARD = True)\n",
    "            k = k1+k2\n",
    "        elif kname == 'MLP+MLP':\n",
    "            k1 = GPy.kern.MLP(input_dim=input_dim, ARD = True)\n",
    "            k2 = GPy.kern.MLP(input_dim = input_dim, ARD=True)\n",
    "            k = k1+k2\n",
    "        elif kname == 'MLP*MLP':\n",
    "            k1 = GPy.kern.MLP(input_dim=input_dim, ARD = True)\n",
    "            k2 = GPy.kern.MLP(input_dim = input_dim, ARD=True)\n",
    "            k = k1*k2\n",
    "        elif kname == 'MLP+EXP':\n",
    "            k1 = GPy.kern.MLP(input_dim=input_dim, ARD = True)\n",
    "            k2 = GPy.kern.Exponential(input_dim = input_dim, ARD=True)\n",
    "            k = k1+k2\n",
    "        elif kname == 'MLP+RQ':\n",
    "            k1 = GPy.kern.MLP(input_dim=input_dim, ARD = True)\n",
    "            k2 = GPy.kern.RatQuad(input_dim = input_dim, ARD=True)\n",
    "            k = k1+k2\n",
    "        elif kname == 'RQ':\n",
    "            k = GPy.kern.RatQuad(input_dim = input_dim, ARD=True)\n",
    "        elif kname == 'Mat32':\n",
    "            k = GPy.kern.Matern32(input_dim = input_dim, ARD=True)\n",
    "        else: \n",
    "            k = None\n",
    "        return k\n",
    "\n",
    "    \n",
    "    def gp_a_dataframe(self, df = None, xcolumns = None, ycolumns = None, kname = None):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        nx = len(xcolumns)\n",
    "        ny = len(ycolumns)\n",
    "        #if (ny != 1):\n",
    "        #    raise Warning(f'y must be 1 dimession!')\n",
    "\n",
    "        if (nx>1):\n",
    "            xx = df[xcolumns].to_numpy()\n",
    "        else:\n",
    "            xx = df[xcolumns].to_numpy().reshape(-1,1)\n",
    "        \n",
    "        if (ny > 1):\n",
    "            yy = df[ycolumns].to_numpy()\n",
    "        else:\n",
    "            yy = df[ycolumns].to_numpy().reshape(-1,1)\n",
    "\n",
    "        kernel = self.kernel_bank(kname, nx)\n",
    "        if (kernel == None):\n",
    "            raise Warning('kname:'+ kanme + ' is not found')\n",
    "            \n",
    "        gpm = GPy.models.GPRegression(xx,yy,kernel)\n",
    "        gpm.optimize()\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        return gpm\n",
    "    \n",
    "    ###################################################\n",
    "    def load_a_gpm(self, df = None, npy = None, xcolumns = None, ycolumns = None, kname = None):\n",
    "        \n",
    "        nx = len(xcolumns)\n",
    "        ny = len(ycolumns)\n",
    "\n",
    "        if (nx>1):\n",
    "            xx = df[xcolumns].to_numpy()\n",
    "        else:\n",
    "            xx = df[xcolumns].to_numpy().reshape(-1,1)\n",
    "        \n",
    "        if (ny > 1):\n",
    "            yy = df[ycolumns].to_numpy()\n",
    "        else:\n",
    "            yy = df[ycolumns].to_numpy().reshape(-1,1)\n",
    "\n",
    "        kernel = self.kernel_bank(kname, nx)\n",
    "        if (kernel == None):\n",
    "            raise Warning('kname:'+ kanme + ' is not found')\n",
    "            \n",
    "        m_load = GPy.models.GPRegression(xx,yy,kernel,initialize=False)\n",
    "        m_load.update_model(False) # do not call the underlying expensive algebra on load\n",
    "        m_load.initialize_parameter() # Initialize the parameters (connect the parameters up)\n",
    "        m_load[:] = np.load(npy) # Load the parameters\n",
    "        m_load.update_model(True) # Call the algebra only once\n",
    "        print(m_load)\n",
    "        return m_load\n",
    "\n",
    "    \n",
    "    ######################inspecting model######################\n",
    "    def inspect_gpm_on_1d(self, gpm = None, df = None, xcolumns = None, ycolumns = None):\n",
    "        nx = len(xcolumns)\n",
    "        ny = len(ycolumns)\n",
    "        if (ny != 1):\n",
    "            raise Warning(f'y must be 1 dimession!')\n",
    "            \n",
    "        ndf = len(df)\n",
    "        \n",
    "        if (nx>1):\n",
    "            xx = df[xcolumns].to_numpy()\n",
    "        else:\n",
    "            xx = df[xcolumns].to_numpy().reshape(-1,1)\n",
    "        \n",
    "        gpy = gpm.predict(xx)\n",
    "        \n",
    "        for name in xcolumns:\n",
    "            fig, ax = plt.subplots(1, figsize=(6,5))\n",
    "            ax.scatter(df[name], df[ycolumns],s = 10, c = 'k')\n",
    "            ax.scatter(df[name], gpy[0].reshape(ndf,), s = 5, c = 'r')\n",
    "            ax.set_xlabel(name)\n",
    "            ax.set_ylabel(ycolumns)\n",
    "        return\n",
    "    \n",
    "        #########################\n",
    "        #####################\n",
    "    \n",
    "    ####\n",
    "    def inspect_gpm_on_2d(self, gpm = None, df = None, xcolumns = None, ycolumns = None, xplots = None,\n",
    "                          randomshift = None, shiftfactor = None, title = None):\n",
    "        nx = len(xcolumns)\n",
    "        ny = len(ycolumns)\n",
    "        if (ny != 1):\n",
    "            raise Warning(f'y must be 1 dimession!')\n",
    "            \n",
    "        ndf = len(df)\n",
    "        \n",
    "        if (nx>1):\n",
    "            xx = df[xcolumns].to_numpy()\n",
    "        else:\n",
    "            xx = df[xcolumns].to_numpy().reshape(-1,1)\n",
    "            \n",
    "        if shiftfactor == None: shiftfactor = 0.01\n",
    "        if randomshift == True: xx = xx + shiftfactor*xx\n",
    "        \n",
    "        gpy = gpm.predict(xx)\n",
    "        \n",
    "        nc = len(xplots)\n",
    "        if nc < 2: raise Warning(f'xplots must be >=2 dimession!')\n",
    "        \n",
    "        for i in range(nc):\n",
    "            name1 = xplots[i]\n",
    "            for i2 in range(nc-i-1):\n",
    "                name2 = xplots[i+i2+1]\n",
    "                print(name1, name2)\n",
    "                fig, (ax1,ax2) = plt.subplots(1,2, figsize=(10,5))\n",
    "                cp = ax1.scatter(df[name1], df[name2], c = (df[ycolumns[0]] - gpy[0].reshape(ndf,))/df[ycolumns[0]],s = 2)\n",
    "                #ax.scatter(df[name1], df[name2], c = gpy[0].reshape(ndf,), s = 15)\n",
    "                ax1.set_xlabel(name1)\n",
    "                ax1.set_ylabel(name2)\n",
    "                colorbar = plt.colorbar(cp)\n",
    "                colorbar.set_label(ycolumns[0] + '(Grid - GP)/Grid')\n",
    "                ax2.hist((df[ycolumns[0]] - gpy[0].reshape(ndf,))/df[ycolumns[0]], bins = 50)\n",
    "                if title != None: ax1.set_title(title)\n",
    "        return\n",
    "\n",
    "    ######################sampling##############################\n",
    "    def guassian_sample(self, obs, obs_e, howmanysigma, n):\n",
    "        x = np.linspace(0,1.0,n)\n",
    "        mu = 0.5\n",
    "        sigma = 1.0/howmanysigma/2.0\n",
    "        y = 1.0/(sigma*(2.0*np.pi)**0.5)*np.exp(-0.5*((x - mu)/sigma)**2.0)\n",
    "        y = y/np.sum(y)\n",
    "        data_array = (np.random.choice(n,n, p = y)/n - 0.5) *(obs_e*howmanysigma*2) + obs\n",
    "        return data_array\n",
    "    \n",
    "    def uniform_sample(self, obs, obs_e, howmanysigma, n):\n",
    "        x = np.linspace(0,1.0,n)\n",
    "        mu = 0.5\n",
    "        sigma = 1.0/howmanysigma/2.0\n",
    "        data_array = (np.random.choice(n,n)/n - 0.5) *(obs_e*howmanysigma*2) + obs\n",
    "        return data_array\n",
    "    \n",
    "    def linear_sample(self, start, end, slope, n):\n",
    "        x = np.linspace(0,1.0,n)\n",
    "        y = slope*x\n",
    "        y = y/np.sum(y)\n",
    "        data_array = (np.random.choice(n,n, p = y)/n) *(end - start) + start\n",
    "        return data_array\n",
    "    \n",
    "    def age_sample(self, start, end, slope, n):\n",
    "        x = np.linspace(0,1.0,n)\n",
    "        y = slope*x**2\n",
    "        y = y/np.sum(y)\n",
    "        data_array = (np.random.choice(n,n, p = y)/n) *(end - start) + start\n",
    "        return data_array\n",
    "    \n",
    "    def mle(self, model, obs, obs_e):\n",
    "        lk = 1.0/ (2.0*3.14159*obs_e**2.0)**(0.5) * np.exp( 0.0 - (model - obs)**2.0/2.0/obs_e**2.0 )\n",
    "        return lk\n",
    "    \n",
    "    def sample_results(self, df = None, lkname = None, f = None):\n",
    "        df[lkname] = df[lkname]/np.sum(df[lkname])\n",
    "        newdf = df.copy()\n",
    "        for index, row in df.iterrows():\n",
    "            copies = int(f*row[lkname])\n",
    "            for i in range(copies):\n",
    "                newdf = newdf.append(row,ignore_index=True)\n",
    "        newdf.drop(index = df.index,inplace=True)\n",
    "        return newdf\n",
    "    \n",
    "    def sample_cmd(self, gpm = None, df = None, prediction = None, \n",
    "               xcolumns = None, xshift = None, randomshift = None,\n",
    "               ccolumns = None, mcolumns = None):\n",
    "        nx = len(xcolumns)\n",
    "        \n",
    "        fig, (ax1,ax2) = plt.subplots(1,2, figsize=(10,5))\n",
    "        ax1.scatter(df[ccolumns], df[mcolumns],c = df[prediction], s = 5)\n",
    "            \n",
    "        ndf = len(df)\n",
    "        \n",
    "        df2 = df.copy()\n",
    "        \n",
    "        for name in xcolumns:\n",
    "            index = xcolumns.index(name)\n",
    "            if (randomshift == True):\n",
    "                df2[name] = df2[name] + xshift[index]*(np.random.random(len(df2[name]))-0.5)\n",
    "            else:\n",
    "                df2[name] = df2[name] + xshift[index]\n",
    "        \n",
    "        if (nx>1):\n",
    "            xx = df2[xcolumns].to_numpy()\n",
    "        else:\n",
    "            xx = df2[xcolumns].to_numpy().reshape(-1,1)\n",
    "        \n",
    "        gpp = gpm.predict(xx)\n",
    "        \n",
    "        ax2.scatter(df2[ccolumns],df2[mcolumns],c = gpp[0].reshape(ndf,), s = 5)\n",
    "        ax1.set_xlabel(ccolumns)\n",
    "        ax1.set_ylabel(mcolumns)\n",
    "        ax1.set_xlim([np.max(df[ccolumns]), np.min(df[ccolumns])])\n",
    "        ax2.set_xlim([np.max(df[ccolumns]), np.min(df[ccolumns])])\n",
    "        ax1.set_ylim([np.max(df[mcolumns]), np.min(df[mcolumns])])\n",
    "        ax2.set_ylim([np.max(df[mcolumns]), np.min(df[mcolumns])])\n",
    "        ax1.set_title('Model Grid')\n",
    "        ax2.set_xlabel(ccolumns)\n",
    "        ax2.set_title('GP predictions')\n",
    "        return \n",
    "    \n",
    "    def sample_with_df(self, n = None, df = None, gpm = None, \n",
    "               xcolumns = None, xshift = None, randomshift = None,\n",
    "               ycolumns = None):\n",
    "        nx = len(xcolumns)\n",
    "        ny = len(ycolumns)\n",
    "        \n",
    "        df2 = df.copy()\n",
    "        if (len(df2) < n):\n",
    "            for i in range(100):\n",
    "                df2 = pd.concat([df2, df])\n",
    "                if len(df2)>n: break\n",
    "        \n",
    "        df2.index = range(len(df2))\n",
    "        \n",
    "        ndf = len(df2)\n",
    "        \n",
    "        for name in xcolumns:\n",
    "            index = xcolumns.index(name)\n",
    "            if (randomshift == True):\n",
    "                df2[name] = df2[name] + xshift[index]*(np.random.random(len(df2[name]))-0.5)\n",
    "            else:\n",
    "                df2[name] = df2[name] + xshift[index]\n",
    "        \n",
    "        if (nx>1):\n",
    "            xx = df2[xcolumns].to_numpy()\n",
    "        else:\n",
    "            xx = df2[xcolumns].to_numpy().reshape(-1,1)\n",
    "        \n",
    "        gpp = gpm.predict(xx)\n",
    "        df2[ycolumns] = gpp[0]\n",
    "        df2['gp_var'] = gpp[1].reshape(ndf,)\n",
    "        \n",
    "        #for yname in ycolumns:\n",
    "        #    index = ycolumns.index(yname)\n",
    "        #    gpp = gpm[index].predict(xx)\n",
    "        #    df2[yname] = gpp[0].reshape(ndf,)\n",
    "        #    df2[yname + '_var'] = gpp[1].reshape(ndf,)\n",
    "        return df2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcolumns = ['initial_mass','frac_age', 'initial_zx', 'initial_Yinit', 'initial_MLT']\n",
    "outputs = ['effective_T','log_g','radius', 'star_age','star_zx', 'delta_nu_fit']\n",
    "xshifts = [0.0,0.0,0.0,0.0,0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path is set as/Users/litz/Documents/GitHub/GPGrid_paper/GPR_model/5d_gpr_models/\n",
      "Validation path is set as/Users/litz/Documents/GitHub/GPGrid_paper/GPR_model/5d_gpr_models/\n",
      "Save path is set as/Users/litz/Documents/GitHub/GPGrid_paper/GPR_model/5d_gpr_models/zoutputs/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.global_md_kernel at 0x7f9915f1d490>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadr = '/Users/litz/Documents/GitHub/GPGrid_paper/GPR_model/5d_gpr_models/'\n",
    "savedr = '/Users/litz/Documents/GitHub/GPGrid_paper/GPR_model/5d_gpr_models/zoutputs/'\n",
    "validr = '/Users/litz/Documents/GitHub/GPGrid_paper/GPR_model/5d_gpr_models/'\n",
    "gmk = global_md_kernel()\n",
    "gmk.set_path(datapath = datadr, savepath = savedr, valipath = validr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_gpms(path, xcolumns, ycolumns, labels, knames, sysfile):\n",
    "    gpms = []\n",
    "    for i in range(len(labels)):\n",
    "        gdf = pd.read_csv(path + ycolumns + '/' + labels[i] + knames[i] + '_gpm_.csv')\n",
    "        npy =  path + ycolumns + '/' + labels[i] + knames[i] + '_gpm_.npy'\n",
    "        gpm = gmk.load_a_gpm(df = gdf, npy = npy, xcolumns = xcolumns, ycolumns = [ycolumns], kname = knames[i])\n",
    "        gpms = np.concatenate((gpms, [gpm]))\n",
    "    sys = pd.read_csv(path + ycolumns + '/' + sysfile)\n",
    "    return gpms,sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /opt/anaconda3/lib/python3.7/site-packages/paramz/parameterized.py:61: RuntimeWarning:Don't forget to initialize by self.initialize_parameter()!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name : GP regression\n",
      "Objective : 45928.36839173853\n",
      "Number of Parameters : 8\n",
      "Number of Optimization Parameters : 8\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mGP_regression.         \u001b[0;0m  |               value  |  constraints  |  priors\n",
      "  \u001b[1mmlp.variance           \u001b[0;0m  |  145855.52137068595  |      +ve      |        \n",
      "  \u001b[1mmlp.weight_variance    \u001b[0;0m  |                (5,)  |      +ve      |        \n",
      "  \u001b[1mmlp.bias_variance      \u001b[0;0m  |  5658.5796025083855  |      +ve      |        \n",
      "  \u001b[1mGaussian_noise.variance\u001b[0;0m  |   27.14203715218965  |      +ve      |        \n",
      "\n",
      "Name : GP regression\n",
      "Objective : 47138.24732479803\n",
      "Number of Parameters : 8\n",
      "Number of Optimization Parameters : 8\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mGP_regression.         \u001b[0;0m  |               value  |  constraints  |  priors\n",
      "  \u001b[1mmlp.variance           \u001b[0;0m  |   63713.42497493717  |      +ve      |        \n",
      "  \u001b[1mmlp.weight_variance    \u001b[0;0m  |                (5,)  |      +ve      |        \n",
      "  \u001b[1mmlp.bias_variance      \u001b[0;0m  |      48217145.21399  |      +ve      |        \n",
      "  \u001b[1mGaussian_noise.variance\u001b[0;0m  |  203.47735907764698  |      +ve      |        \n",
      "\n",
      "Name : GP regression\n",
      "Objective : 47522.981058249876\n",
      "Number of Parameters : 7\n",
      "Number of Optimization Parameters : 7\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mGP_regression.         \u001b[0;0m  |               value  |  constraints  |  priors\n",
      "  \u001b[1mrbf.variance           \u001b[0;0m  |  103.82953959969022  |      +ve      |        \n",
      "  \u001b[1mrbf.lengthscale        \u001b[0;0m  |                (5,)  |      +ve      |        \n",
      "  \u001b[1mGaussian_noise.variance\u001b[0;0m  |  107.65443701043418  |      +ve      |        \n"
     ]
    }
   ],
   "source": [
    "ycolumns = outputs[0]\n",
    "gpms_teff,sys_teff = import_gpms(datadr, xcolumns,ycolumns, ['M0_','M1_','M2_'], \n",
    "                                 knames = ['MLP', 'MLP', 'RBF'],\n",
    "                                 sysfile = 'M2_RBF_offsets_.csv'\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name : GP regression\n",
      "Objective : -39413.92947736647\n",
      "Number of Parameters : 8\n",
      "Number of Optimization Parameters : 8\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mGP_regression.         \u001b[0;0m  |                  value  |  constraints  |  priors\n",
      "  \u001b[1mmlp.variance           \u001b[0;0m  |      820.9061774683148  |      +ve      |        \n",
      "  \u001b[1mmlp.weight_variance    \u001b[0;0m  |                   (5,)  |      +ve      |        \n",
      "  \u001b[1mmlp.bias_variance      \u001b[0;0m  |      1237.750789094949  |      +ve      |        \n",
      "  \u001b[1mGaussian_noise.variance\u001b[0;0m  |  1.044615189560547e-05  |      +ve      |        \n",
      "\n",
      "Name : GP regression\n",
      "Objective : -37334.288891299504\n",
      "Number of Parameters : 7\n",
      "Number of Optimization Parameters : 7\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mGP_regression.         \u001b[0;0m  |                   value  |  constraints  |  priors\n",
      "  \u001b[1mMat32.variance         \u001b[0;0m  |  4.8395809376808976e-05  |      +ve      |        \n",
      "  \u001b[1mMat32.lengthscale      \u001b[0;0m  |                    (5,)  |      +ve      |        \n",
      "  \u001b[1mGaussian_noise.variance\u001b[0;0m  |   3.322057865998519e-16  |      +ve      |        \n",
      "\n",
      "Name : GP regression\n",
      "Objective : -39825.01470454389\n",
      "Number of Parameters : 7\n",
      "Number of Optimization Parameters : 7\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mGP_regression.         \u001b[0;0m  |                  value  |  constraints  |  priors\n",
      "  \u001b[1mExponential.variance   \u001b[0;0m  |  2.248227571454475e-05  |      +ve      |        \n",
      "  \u001b[1mExponential.lengthscale\u001b[0;0m  |                   (5,)  |      +ve      |        \n",
      "  \u001b[1mGaussian_noise.variance\u001b[0;0m  |  4.179738363563105e-11  |      +ve      |        \n"
     ]
    }
   ],
   "source": [
    "ycolumns = outputs[1]\n",
    "gpms_logg,sys_logg = import_gpms(datadr, xcolumns, ycolumns, ['M0_','M1_','M2_'], \n",
    "                                 knames = ['MLP', 'Mat32', 'EXP'],\n",
    "                                 sysfile = 'M2_EXP_offsets_.csv'\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name : GP regression\n",
      "Objective : -72022.4893602153\n",
      "Number of Parameters : 8\n",
      "Number of Optimization Parameters : 8\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mGP_regression.         \u001b[0;0m  |                  value  |  constraints  |  priors\n",
      "  \u001b[1mmlp.variance           \u001b[0;0m  |     428.36065381970286  |      +ve      |        \n",
      "  \u001b[1mmlp.weight_variance    \u001b[0;0m  |                   (5,)  |      +ve      |        \n",
      "  \u001b[1mmlp.bias_variance      \u001b[0;0m  |     175.32794541526178  |      +ve      |        \n",
      "  \u001b[1mGaussian_noise.variance\u001b[0;0m  |  3.128989157763501e-09  |      +ve      |        \n"
     ]
    }
   ],
   "source": [
    "ycolumns = outputs[4]\n",
    "gpms_zx,sys_zx = import_gpms(datadr, xcolumns, ycolumns, ['M0_','M1_','M2_'], \n",
    "                             knames = ['MLP', 'Mat32', 'RBF'],\n",
    "                             sysfile = 'M2_RBF_offsets_.csv'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "mass = np.random.uniform(low=0.81, high=1.19, size=(n,1))\n",
    "fage = np.random.uniform(low=2.0, high=9.9, size=(n,1))\n",
    "fehinit = np.random.uniform(low=0.0, high=0.0, size=(n,1))\n",
    "zxinit = 10**fehinit*0.0181\n",
    "yinit = np.random.uniform(low=0.28, high=0.28, size=(n,1))\n",
    "mlt = np.random.uniform(low=2.1, high=2.1, size=(n,1))\n",
    "data = np.concatenate((mass, fage, zxinit,yinit,mlt),axis = 1)\n",
    "sdf = pd.DataFrame(data = data, columns = xcolumns)\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(sdf, gpms, nmodels,  xcolumns, ycolumns):\n",
    "    print('inputs:', xcolumns)\n",
    "    print('outputs', ycolumns)\n",
    "    gpp = sdf.copy()\n",
    "    gpps = sdf.copy()\n",
    "    for i in range(nmodels):\n",
    "        gpp = gmk.sample_with_df(n = 1, df = sdf, gpm = gpms[i], \n",
    "                                 xcolumns = xcolumns, xshift = xshifts, \n",
    "                                 randomshift = False, ycolumns = ycolumns)\n",
    "        if i == 0: gpps[ycolumns] = gpp[ycolumns]\n",
    "        if i > 0: gpps[ycolumns] = gpps[ycolumns] + gpp[ycolumns]\n",
    "    return gpps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpp_teff = make_predictions(sdf, gpms_teff, 3, xcolumns, 'effective_T')\n",
    "gpp_logg = make_predictions(sdf, gpms_logg, 3, xcolumns, 'log_g')\n",
    "gpp_zx = make_predictions(sdf, gpms_logg, 3, xcolumns, 'star_zx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "cp = ax.scatter(gpp_teff['effective_T'],gpp_logg['log_g'], c = np.log10(gpp_zx['star_zx']/0.0181),s =10)\n",
    "ax.set_xlabel(r'$T_{\\rm eff}$ (K)')\n",
    "ax.set_ylabel(r'$\\log g$')\n",
    "ax.set_xlim([6600,4750])\n",
    "ax.set_ylim([4.7,3.55])\n",
    "ax.set_title('GP predictions')\n",
    "#ax.legend(fontsize = 20)\n",
    "cc = plt.colorbar(cp)\n",
    "cc.set_label(r'[Fe/H]')\n",
    "fig.tight_layout()\n",
    "#fig.savefig(gmk._savepath +'off_validation_hr.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
